\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{kappes2011globally}
\citation{chopra1991multiway}
\citation{lange2018combinatorial}
\citation{pape2017solving}
\citation{beier2016efficient}
\citation{yarkony2012fast}
\citation{keuper2015efficient}
\citation{levinkov2017comparative}
\citation{wolf2018mutex}
\citation{kardoostsolving}
\citation{lance1967general}
\citation{liu2018affinity}
\citation{lee2017superhuman}
\citation{wolf2018mutex}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{brf}{\backcite{kappes2011globally,chopra1991multiway}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{lange2018combinatorial,pape2017solving,beier2016efficient,yarkony2012fast}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{keuper2015efficient,levinkov2017comparative,wolf2018mutex,kardoostsolving}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{lance1967general}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{liu2018affinity,lee2017superhuman,wolf2018mutex}{{1}{1}{section.1}}}
\citation{lin2014microsoft}
\citation{everingham2010pascal}
\citation{cordts2016cityscapes}
\citation{he2017mask}
\citation{porzi2019seamless}
\citation{liu2018path}
\citation{yang2012layered}
\citation{li2017fully}
\citation{ladicky2010and}
\citation{hariharan2014simultaneous}
\citation{chen2015multi}
\citation{dai2016instance}
\citation{liang2016reversible}
\citation{romera2016recurrent}
\citation{ren2017end}
\citation{kirillov2017instancecut}
\citation{bai2017deep}
\citation{lee2019learning}
\citation{fathi2017semantic}
\citation{newell2017associative}
\citation{de2017semantic}
\citation{kong2018recurrentPix}
\citation{neven2019instance}
\citation{cheng2019panopticdeeplab}
\citation{sofiiuk2019adaptis}
\citation{Gao_2019_ICCV}
\citation{liu2018affinity}
\citation{xie2015holistically}
\citation{kokkinos2015pushing}
\citation{lee2017superhuman}
\citation{schmidt2018cell}
\citation{meirovitch2016multi}
\citation{ciresan2012deep}
\citation{kaynig2015large}
\citation{krasowski2015improving}
\citation{meirovitch2016multi}
\citation{liu2016sshmt}
\citation{liu2014modular}
\citation{funke2015learning}
\citation{uzunbas2016efficient}
\citation{beier2017multicut}
\citation{januszewski2018high}
\citation{meirovitch2016multi}
\citation{meirovitch2019cross}
\citation{funke2018large}
\citation{turaga2009maximin}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces GASP{} example. \textbf  {(1)} Raw data from the CREMI 2016 neuron-segmentation challenge. \textbf  {(2)} Some short- and long-range predictions of our CNN model, where white pixels represent boundary evidence. \textbf  {(3)} Outputs of two agglomerative algorithms included in our proposed generalized clustering framework, with \emph  {Sum} and \emph  {Average} linkage criteria. The final clustering / instance segmentation is shown in 3a, overlaid with the raw image. The agglomeration order in 3b shows which pairs of neighboring pixels were merged first (white), later on (brown/red), or never (black). (\textbf  {4}) Some iterations of GASP{} on toy graph examples with attractive/positive (green) and repulsive/negative (red) interactions. At each iteration, the yellow edge with highest interaction is contracted (orange arrows), until only negative edges are left in the graph. \relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro_figure}{{1}{2}{\algname {} example. \textbf {(1)} Raw data from the CREMI 2016 neuron-segmentation challenge. \textbf {(2)} Some short- and long-range predictions of our CNN model, where white pixels represent boundary evidence. \textbf {(3)} Outputs of two agglomerative algorithms included in our proposed generalized clustering framework, with \emph {Sum} and \emph {Average} linkage criteria. The final clustering / instance segmentation is shown in 3a, overlaid with the raw image. The agglomeration order in 3b shows which pairs of neighboring pixels were merged first (white), later on (brown/red), or never (black). (\textbf {4}) Some iterations of \algname {} on toy graph examples with attractive/positive (green) and repulsive/negative (red) interactions. At each iteration, the yellow edge with highest interaction is contracted (orange arrows), until only negative edges are left in the graph. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related work}{2}{section.2}}
\newlabel{sec:related_work}{{2}{2}{\hskip -1em.~Related work}{section.2}{}}
\@writefile{brf}{\backcite{lin2014microsoft}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{everingham2010pascal}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{cordts2016cityscapes}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{he2017mask,porzi2019seamless,liu2018path,yang2012layered,li2017fully,ladicky2010and,hariharan2014simultaneous,chen2015multi,dai2016instance,liang2016reversible}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{romera2016recurrent,ren2017end}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{kirillov2017instancecut}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{bai2017deep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{lee2019learning,fathi2017semantic,newell2017associative,de2017semantic}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{kong2018recurrentPix}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{neven2019instance,cheng2019panopticdeeplab}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{sofiiuk2019adaptis}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{Gao_2019_ICCV,liu2018affinity,xie2015holistically,kokkinos2015pushing}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{lee2017superhuman,schmidt2018cell,meirovitch2016multi,ciresan2012deep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{kaynig2015large,krasowski2015improving}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{meirovitch2016multi,liu2016sshmt,liu2014modular,funke2015learning,uzunbas2016efficient}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{beier2017multicut}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{januszewski2018high}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{meirovitch2016multi}{{2}{2}{section.2}}}
\citation{arbelaez2011contour}
\citation{ren2013image}
\citation{liu2016image}
\citation{salembier2000binary}
\citation{malmberg2011generalized}
\citation{felzenszwalb2004efficient}
\citation{kiran2014global}
\citation{liu2018affinity}
\citation{lee2017superhuman}
\citation{funke2018large}
\citation{nunez2013machine}
\citation{knowles2016rhoananet}
\citation{grotschel1989cutting}
\citation{grotschel1990facets}
\citation{chopra1993partition}
\citation{bansal2004correlation}
\citation{demaine2006correlation}
\citation{andres2012globally}
\citation{pape2017solving}
\citation{beier2016efficient}
\citation{yarkony2012fast}
\citation{levinkov2017comparative}
\citation{wolf2019mutex}
\citation{keuper2015efficient}
\citation{kardoostsolving}
\citation{lange2018partial}
\citation{lange2018combinatorial}
\citation{Cucuringu2019SPONGEAG}
\citation{chiang2012scalable}
\citation{kunegis2010spectral}
\citation{rangapuram2012constrained}
\citation{wang2014constrained}
\citation{cucuringu2016simple}
\citation{levinkov2017comparative}
\citation{wolf2018mutex}
\citation{keuper2015efficient}
\citation{liu2018affinity}
\citation{wolf2018mutex}
\citation{lee2017superhuman}
\citation{lange2018partial}
\@writefile{brf}{\backcite{meirovitch2019cross}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{funke2018large,turaga2009maximin}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{arbelaez2011contour,ren2013image,liu2016image,salembier2000binary}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{malmberg2011generalized}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{felzenszwalb2004efficient}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{kiran2014global}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{liu2018affinity,lee2017superhuman}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{funke2018large}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{nunez2013machine,knowles2016rhoananet}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{grotschel1989cutting,grotschel1990facets,chopra1993partition}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{bansal2004correlation}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{demaine2006correlation}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{andres2012globally}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{pape2017solving,beier2016efficient,yarkony2012fast}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{levinkov2017comparative,wolf2019mutex,keuper2015efficient,kardoostsolving}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{lange2018partial,lange2018combinatorial}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{Cucuringu2019SPONGEAG,chiang2012scalable,kunegis2010spectral}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{rangapuram2012constrained,wang2014constrained,cucuringu2016simple}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{levinkov2017comparative,wolf2018mutex,keuper2015efficient}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{liu2018affinity,wolf2018mutex,lee2017superhuman}{{3}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Generalized framework for agglomerative clustering of signed graphs}{3}{section.3}}
\newlabel{sec:general_framework}{{3}{3}{\hskip -1em.~Generalized framework for agglomerative clustering of signed graphs}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Notation and graph formalism}{3}{subsection.3.1}}
\newlabel{sec:notation}{{3.1}{3}{\hskip -1em.~Notation and graph formalism}{subsection.3.1}{}}
\@writefile{brf}{\backcite{lange2018partial}{{3}{3.1}{subsection.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}GASP{}: generalized algorithm for signed graph partitioning}{3}{subsection.3.2}}
\newlabel{sec:algorithm}{{3.2}{3}{\hskip -1em.~\algname {}: generalized algorithm for signed graph partitioning}{subsection.3.2}{}}
\citation{levinkov2017comparative}
\citation{keuper2015efficient}
\citation{wolf2018mutex}
\citation{nunez2013machine}
\citation{felzenszwalb2004efficient}
\citation{kardoostsolving}
\citation{funke2018large}
\citation{keuper2015efficient}
\citation{wolf2018mutex}
\citation{levinkov2017comparative}
\citation{wolf2018mutex}
\citation{schlegel2017learning}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces GASP{}: generalized algorithm for signed graph partitioning\relax }}{4}{algorithm.1}}
\newlabel{main_alg}{{1}{4}{\algname {}: generalized algorithm for signed graph partitioning\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}GASP{} with different linkage criteria: new and existing algorithms}{4}{subsection.3.3}}
\newlabel{sec:alg_update_rules}{{3.3}{4}{\hskip -1em.~\algname {} with different linkage criteria: new and existing algorithms}{subsection.3.3}{}}
\@writefile{brf}{\backcite{levinkov2017comparative,keuper2015efficient}{{4}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{4}{3.3}{subsection.3.3}}}
\newlabel{subfig:no_constraints}{{2(a)}{4}{No constraints\relax }{figure.caption.2}{}}
\newlabel{sub@subfig:no_constraints}{{(a)}{4}{No constraints\relax }{figure.caption.2}{}}
\newlabel{subfig:with_constraints}{{2(b)}{4}{With cannot-link constraints\relax }{figure.caption.2}{}}
\newlabel{sub@subfig:with_constraints}{{(b)}{4}{With cannot-link constraints\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Some iterations of the generalized algorithm (using \emph  {Sum} linkage criteria) with and without adding cannot-link constraints. The graph has both attractive (green) and repulsive (red) edges and cannot-link constraints are shown with triple violet bars on the edges. The edge selected at each iteration is highlighted in yellow. We note that when constraints are enforced, the final clustering is given by two clusters instead of only one.\relax }}{4}{figure.caption.2}}
\newlabel{fig:algorithm_with_without_CLC}{{2}{4}{Some iterations of the generalized algorithm (using \emph {Sum} linkage criteria) with and without adding cannot-link constraints. The graph has both attractive (green) and repulsive (red) edges and cannot-link constraints are shown with triple violet bars on the edges. The edge selected at each iteration is highlighted in yellow. We note that when constraints are enforced, the final clustering is given by two clusters instead of only one.\relax }{figure.caption.2}{}}
\@writefile{brf}{\backcite{nunez2013machine}{{4}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{felzenszwalb2004efficient,kardoostsolving}{{4}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{funke2018large}{{4}{3.3}{subsection.3.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments on neuron segmentation}{4}{section.4}}
\newlabel{sec:neuro_segm_exp}{{4}{4}{\hskip -1em.~Experiments on neuron segmentation}{section.4}{}}
\citation{beier2017multicut}
\citation{ciresan2012deep}
\citation{wolf2018mutex}
\citation{lee2017superhuman}
\citation{funke2018large}
\citation{lee2017superhuman}
\citation{levinkov2017comparative}
\citation{wolf2018mutex}
\citation{keuper2015efficient}
\citation{lee2017superhuman}
\citation{funke2018large}
\citation{zeng2017deepem3d}
\citation{parag2017anisotropic}
\citation{cremiChallenge}
\citation{cremiChallenge}
\citation{cheng2019panopticdeeplab}
\citation{xiong2019upsnet}
\citation{Gao_2019_ICCV}
\citation{sofiiuk2019adaptis}
\citation{liu2018path}
\citation{liu2018affinity}
\citation{neven2019instance}
\citation{liu2018affinity}
\citation{he2017mask}
\citation{liu2017sgn}
\citation{liu2018affinity}
\citation{wolf2018mutex}
\citation{levinkov2017comparative}
\citation{keuper2015efficient}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{cremiChallenge}
\citation{arganda2015crowdsourcing}
\citation{lee2017superhuman}
\citation{funke2018large}
\citation{keuper2015efficient}
\citation{levinkov2017comparative}
\citation{wolf2018mutex}
\citation{beier2017multicut}
\@writefile{brf}{\backcite{keuper2015efficient}{{5}{\caption@xref {??}{ on input line 150}}{table.caption.3}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{5}{\caption@xref {??}{ on input line 150}}{table.caption.3}}}
\@writefile{brf}{\backcite{levinkov2017comparative}{{5}{\caption@xref {??}{ on input line 151}}{table.caption.3}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{5}{\caption@xref {??}{ on input line 151}}{table.caption.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Existing and new clustering algorithms that can be reformulated as special cases of the proposed generalized algorithm for signed graph partitioning, GASP{}, given a linkage criterion, a type of graph (signed or unsigned) and the optional use of cannot-link constraints. The set $E_{uv}$ is defined as the set of all edges connecting cluster $S_u$ to cluster $S_v$, i.e. $E_{uv}=(S_u \times S_{v \not =u}) \cap E$.\relax }}{5}{table.caption.3}}
\newlabel{tab:linkage-criteria}{{1}{5}{Existing and new clustering algorithms that can be reformulated as special cases of the proposed generalized algorithm for signed graph partitioning, \algname {}, given a linkage criterion, a type of graph (signed or unsigned) and the optional use of cannot-link constraints. The set $E_{uv}$ is defined as the set of all edges connecting cluster $S_u$ to cluster $S_v$, i.e. $E_{uv}=(S_u \times S_{v \neq u}) \cap E$.\relax }{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Failure cases of GASP{} with different linkage criteria highlighted on some difficult parts of the CREMI Challenge data. Only the \emph  {wrongly} segmented regions are highlighted in different warm colors. Note that the data is 3D, hence the same color could be assigned to parts of segments that appear disconnected in 2D. Red arrows point to wrongly split regions. Yellow arrows point out merge errors. The \emph  {Average} linkage without cannot-link constraints returned the best segmentation. \relax }}{5}{figure.caption.4}}
\newlabel{fig:cremi_comparison}{{3}{5}{Failure cases of \algname {} with different linkage criteria highlighted on some difficult parts of the CREMI Challenge data. Only the \emph {wrongly} segmented regions are highlighted in different warm colors. Note that the data is 3D, hence the same color could be assigned to parts of segments that appear disconnected in 2D. Red arrows point to wrongly split regions. Yellow arrows point out merge errors. The \emph {Average} linkage without cannot-link constraints returned the best segmentation. \relax }{figure.caption.4}{}}
\@writefile{brf}{\backcite{schlegel2017learning}{{5}{4}{section.4}}}
\@writefile{brf}{\backcite{beier2017multicut,ciresan2012deep}{{5}{4}{section.4}}}
\@writefile{brf}{\backcite{wolf2018mutex,lee2017superhuman,funke2018large}{{5}{4}{section.4}}}
\@writefile{brf}{\backcite{lee2017superhuman}{{5}{4}{section.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Data: CREMI challenge}{5}{subsection.4.1}}
\newlabel{sec:cremi_challenge}{{4.1}{5}{\hskip -1em.~Data: CREMI challenge}{subsection.4.1}{}}
\@writefile{brf}{\backcite{cremiChallenge}{{5}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{arganda2015crowdsourcing}{{5}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{lee2017superhuman,funke2018large}{{5}{4.1}{subsection.4.1}}}
\newlabel{fig:merge_noise_only_direct}{{4(a)}{6}{No long-range predictions: $p_{\mathrm {long}}=0$\relax }{figure.caption.5}{}}
\newlabel{sub@fig:merge_noise_only_direct}{{(a)}{6}{No long-range predictions: $p_{\mathrm {long}}=0$\relax }{figure.caption.5}{}}
\newlabel{fig:merge_noise_with_long_range}{{4(b)}{6}{With long-range predictions: $p_{\mathrm {long}}=0.1$\relax }{figure.caption.5}{}}
\newlabel{sub@fig:merge_noise_with_long_range}{{(b)}{6}{With long-range predictions: $p_{\mathrm {long}}=0.1$\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces GASP{} sensitivity to noise: \emph  {Average} linkage proved to be the most robust. Performances are given by Rand-Score (higher is better) depending on the amount of noise added to the CNN predictions. Solid lines represent median values over 30 experiments. Values between the 25th and the 75th percentile are shown in shaded areas. The two sets of experiments using under- and over-clustering noise are summarized in the plots at the top and at the bottom, respectively (see Appendix \ref  {sec:appendix_noise_gen} for more details). For each experiment, some of the long-range CNN predictions were randomly selected with probability $p_{\mathrm  {long}}$ and added as long-range edges to the pixel grid-graph. Experiments are performed on a crop of CREMI training sample B. \relax }}{6}{figure.caption.5}}
\newlabel{fig:noise_plots}{{4}{6}{\algname {} sensitivity to noise: \emph {Average} linkage proved to be the most robust. Performances are given by Rand-Score (higher is better) depending on the amount of noise added to the CNN predictions. Solid lines represent median values over 30 experiments. Values between the 25th and the 75th percentile are shown in shaded areas. The two sets of experiments using under- and over-clustering noise are summarized in the plots at the top and at the bottom, respectively (see Appendix \ref {sec:appendix_noise_gen} for more details). For each experiment, some of the long-range CNN predictions were randomly selected with probability $p_{\mathrm {long}}$ and added as long-range edges to the pixel grid-graph. Experiments are performed on a crop of CREMI training sample B. \relax }{figure.caption.5}{}}
\@writefile{brf}{\backcite{levinkov2017comparative}{{6}{\caption@xref {??}{ on input line 36}}{figure.caption.6}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{6}{\caption@xref {??}{ on input line 37}}{figure.caption.6}}}
\@writefile{brf}{\backcite{keuper2015efficient}{{6}{\caption@xref {??}{ on input line 39}}{figure.caption.6}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces CREMI-Scores achieved by different linkage criteria and thresholding. All methods use the affinity predictions from our CNN as input. Scores are averaged over the three CREMI training datasets.\relax }}{6}{figure.caption.6}}
\newlabel{tab:results_cremi_train}{{2}{6}{CREMI-Scores achieved by different linkage criteria and thresholding. All methods use the affinity predictions from our CNN as input. Scores are averaged over the three CREMI training datasets.\relax }{figure.caption.6}{}}
\@writefile{brf}{\backcite{lee2017superhuman}{{6}{\caption@xref {??}{ on input line 52}}{figure.caption.6}}}
\@writefile{brf}{\backcite{funke2018large}{{6}{\caption@xref {??}{ on input line 54}}{figure.caption.6}}}
\@writefile{brf}{\backcite{zeng2017deepem3d}{{6}{\caption@xref {??}{ on input line 55}}{figure.caption.6}}}
\@writefile{brf}{\backcite{parag2017anisotropic}{{6}{\caption@xref {??}{ on input line 56}}{figure.caption.6}}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Current leading entries in the CREMI challenge leaderboard \cite  {cremiChallenge} (November 2019). All entries, apart from our using GASP{}, employ superpixel-based post-processing pipelines.\relax }}{6}{figure.caption.6}}
\@writefile{brf}{\backcite{cremiChallenge}{{6}{3}{figure.caption.6}}}
\newlabel{tab:results_cremi_test}{{3}{6}{Current leading entries in the CREMI challenge leaderboard \cite {cremiChallenge} (November 2019). All entries, apart from our using \algname {}, employ superpixel-based post-processing pipelines.\relax }{figure.caption.6}{}}
\@writefile{brf}{\backcite{cheng2019panopticdeeplab}{{6}{\caption@xref {??}{ on input line 69}}{figure.caption.6}}}
\@writefile{brf}{\backcite{xiong2019upsnet}{{6}{\caption@xref {??}{ on input line 70}}{figure.caption.6}}}
\@writefile{brf}{\backcite{Gao_2019_ICCV}{{6}{\caption@xref {??}{ on input line 71}}{figure.caption.6}}}
\@writefile{brf}{\backcite{sofiiuk2019adaptis}{{6}{\caption@xref {??}{ on input line 72}}{figure.caption.6}}}
\@writefile{brf}{\backcite{liu2018path}{{6}{\caption@xref {??}{ on input line 73}}{figure.caption.6}}}
\@writefile{brf}{\backcite{liu2018affinity}{{6}{\caption@xref {??}{ on input line 74}}{figure.caption.6}}}
\@writefile{brf}{\backcite{neven2019instance}{{6}{\caption@xref {??}{ on input line 75}}{figure.caption.6}}}
\@writefile{brf}{\backcite{liu2018affinity}{{6}{\caption@xref {??}{ on input line 76}}{figure.caption.6}}}
\@writefile{brf}{\backcite{he2017mask}{{6}{\caption@xref {??}{ on input line 77}}{figure.caption.6}}}
\@writefile{brf}{\backcite{liu2017sgn}{{6}{\caption@xref {??}{ on input line 78}}{figure.caption.6}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results on CityScapes test. Methods marked with\nobreakspace  {}$\dagger $ are \emph  {proposal-based}. Only methods that do not use external training data (such as MS COCO) are shown.\relax }}{6}{figure.caption.6}}
\newlabel{tab:results_cityscapes}{{4}{6}{Results on CityScapes test. Methods marked with~$\dagger $ are \emph {proposal-based}. Only methods that do not use external training data (such as MS COCO) are shown.\relax }{figure.caption.6}{}}
\newlabel{tab:results_cityscapes_test}{{4}{6}{Results on CityScapes test. Methods marked with~$\dagger $ are \emph {proposal-based}. Only methods that do not use external training data (such as MS COCO) are shown.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Results and discussion}{6}{subsection.4.2}}
\newlabel{sec:results}{{4.2}{6}{\hskip -1em.~Results and discussion}{subsection.4.2}{}}
\@writefile{brf}{\backcite{keuper2015efficient}{{6}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{levinkov2017comparative}{{6}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{6}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{beier2017multicut}{{6}{4.2}{subsection.4.2}}}
\citation{wolf2018mutex}
\citation{cordts2016cityscapes}
\citation{Gao_2019_ICCV}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{wolf2018mutex}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visual results given by different GASP{} linkage criteria on a crop of a CityScapes image from the \emph  {validation} set.\relax }}{7}{figure.caption.7}}
\newlabel{fig:cityscapes}{{5}{7}{Visual results given by different \algname {} linkage criteria on a crop of a CityScapes image from the \emph {validation} set.\relax }{figure.caption.7}{}}
\@writefile{brf}{\backcite{liu2018affinity}{{7}{\caption@xref {??}{ on input line 107}}{figure.caption.7}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{7}{\caption@xref {??}{ on input line 108}}{figure.caption.7}}}
\@writefile{brf}{\backcite{levinkov2017comparative}{{7}{\caption@xref {??}{ on input line 109}}{figure.caption.7}}}
\@writefile{brf}{\backcite{keuper2015efficient}{{7}{\caption@xref {??}{ on input line 110}}{figure.caption.7}}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Average Precision scores on CityScapes \emph  {val} achieved by the GMIS Model trained in \cite  {liu2018affinity} and different graph agglomeration methods.\relax }}{7}{figure.caption.7}}
\@writefile{brf}{\backcite{liu2018affinity}{{7}{5}{figure.caption.7}}}
\newlabel{tab:results_cityscapes_val}{{5}{7}{Average Precision scores on CityScapes \emph {val} achieved by the GMIS Model trained in \cite {liu2018affinity} and different graph agglomeration methods.\relax }{figure.caption.7}{}}
\@writefile{brf}{\backcite{wolf2018mutex}{{7}{4.2}{subsection.4.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Experiments on CityScapes}{7}{section.5}}
\newlabel{sec:cityscapes_exp}{{5}{7}{\hskip -1em.~Experiments on CityScapes}{section.5}{}}
\@writefile{brf}{\backcite{cordts2016cityscapes}{{7}{5}{section.5}}}
\@writefile{brf}{\backcite{Gao_2019_ICCV}{{7}{5}{section.5}}}
\@writefile{brf}{\backcite{liu2018affinity}{{7}{5}{section.5}}}
\@writefile{brf}{\backcite{liu2018affinity}{{7}{5}{section.5}}}
\@writefile{brf}{\backcite{liu2018affinity}{{7}{5}{section.5}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{7}{5}{section.5}}}
\citation{cheng2019panopticdeeplab}
\citation{Gao_2019_ICCV}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{wolf2018mutex}
\bibstyle{ieee_fullname}
\bibdata{agglo_clust}
\bibcite{ailon2008aggregating}{1}
\bibcite{andres2012globally}{2}
\bibcite{arbelaez2011contour}{3}
\bibcite{arganda2015crowdsourcing}{4}
\bibcite{bai2017deep}{5}
\bibcite{bansal2004correlation}{6}
\bibcite{beier2016efficient}{7}
\bibcite{beier2017multicut}{8}
\bibcite{chen2015multi}{9}
\bibcite{cheng2019panopticdeeplab}{10}
\bibcite{chiang2012scalable}{11}
\bibcite{chopra1991multiway}{12}
\bibcite{chopra1993partition}{13}
\bibcite{cciccek20163d}{14}
\bibcite{ciresan2012deep}{15}
\bibcite{cordts2016cityscapes}{16}
\@writefile{brf}{\backcite{cheng2019panopticdeeplab}{{8}{5}{section.5}}}
\@writefile{brf}{\backcite{Gao_2019_ICCV}{{8}{5}{section.5}}}
\@writefile{brf}{\backcite{liu2018affinity}{{8}{5}{section.5}}}
\@writefile{brf}{\backcite{liu2018affinity}{{8}{5}{section.5}}}
\@writefile{brf}{\backcite{liu2018affinity}{{8}{5}{section.5}}}
\@writefile{brf}{\backcite{liu2018affinity}{{8}{5}{section.5}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Conclusion}{8}{section.6}}
\@writefile{brf}{\backcite{wolf2018mutex}{{8}{6}{section.6}}}
\bibcite{Cucuringu2019SPONGEAG}{17}
\bibcite{cucuringu2016simple}{18}
\bibcite{dai2016instance}{19}
\bibcite{de2017semantic}{20}
\bibcite{demaine2006correlation}{21}
\bibcite{dice1945measures}{22}
\bibcite{everingham2010pascal}{23}
\bibcite{fathi2017semantic}{24}
\bibcite{felzenszwalb2004efficient}{25}
\bibcite{finkel2008enforcing}{26}
\bibcite{funke2015learning}{27}
\bibcite{cremiChallenge}{28}
\bibcite{funke2018large}{29}
\bibcite{Gao_2019_ICCV}{30}
\bibcite{grotschel1989cutting}{31}
\bibcite{grotschel1990facets}{32}
\bibcite{hariharan2014simultaneous}{33}
\bibcite{he2017mask}{34}
\bibcite{januszewski2018high}{35}
\bibcite{kappes2011globally}{36}
\bibcite{kardoostsolving}{37}
\bibcite{kaynig2015large}{38}
\bibcite{kernighan1970efficient}{39}
\bibcite{keuper2015efficient}{40}
\bibcite{kiran2014global}{41}
\bibcite{kirillov2017instancecut}{42}
\bibcite{knowles2016rhoananet}{43}
\bibcite{kokkinos2015pushing}{44}
\bibcite{kong2018recurrentPix}{45}
\bibcite{krasowski2015improving}{46}
\bibcite{kunegis2010spectral}{47}
\bibcite{ladicky2010and}{48}
\bibcite{lance1967general}{49}
\bibcite{lange2018combinatorial}{50}
\bibcite{lange2018partial}{51}
\bibcite{lee2019learning}{52}
\bibcite{lee2017superhuman}{53}
\bibcite{levinkov2017comparative}{54}
\bibcite{li2017fully}{55}
\bibcite{liang2016reversible}{56}
\bibcite{lin2014microsoft}{57}
\bibcite{liu2017sgn}{58}
\bibcite{liu2018path}{59}
\bibcite{liu2014modular}{60}
\bibcite{liu2016image}{61}
\bibcite{liu2016sshmt}{62}
\bibcite{liu2018affinity}{63}
\bibcite{malmberg2011generalized}{64}
\bibcite{meirovitch2016multi}{65}
\bibcite{meirovitch2019cross}{66}
\bibcite{neven2019instance}{67}
\bibcite{newell2017associative}{68}
\bibcite{nunez2013machine}{69}
\bibcite{pape2017solving}{70}
\bibcite{parag2017anisotropic}{71}
\bibcite{perlin1985image}{72}
\bibcite{perlin2001noise}{73}
\bibcite{porzi2019seamless}{74}
\bibcite{rangapuram2012constrained}{75}
\bibcite{ren2017end}{76}
\bibcite{ren2013image}{77}
\bibcite{romera2016recurrent}{78}
\bibcite{ronneberger2015u}{79}
\bibcite{saalfeld2012elastic}{80}
\bibcite{salembier2000binary}{81}
\bibcite{schlegel2017learning}{82}
\bibcite{schmidt2018cell}{83}
\bibcite{sofiiuk2019adaptis}{84}
\bibcite{sorensen1948method}{85}
\bibcite{turaga2009maximin}{86}
\bibcite{uzunbas2016efficient}{87}
\bibcite{wang2014constrained}{88}
\bibcite{wolf2019mutex}{89}
\bibcite{wolf2018mutex}{90}
\bibcite{xie2015holistically}{91}
\bibcite{xiong2019upsnet}{92}
\bibcite{yang2012layered}{93}
\bibcite{yarkony2012fast}{94}
\bibcite{zeng2017deepem3d}{95}
\citation{wolf2018mutex}
\citation{wolf2018mutex}
\citation{wolf2018mutex}
\citation{wolf2018mutex}
\@writefile{toc}{\contentsline {section}{\numberline {7}\hskip -1em.\nobreakspace  {}Supplementary material}{12}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}\hskip -1em.\nobreakspace  {}Implementation details and complexity of GASP{}}{12}{subsection.7.1}}
\newlabel{sec:detailed_impl}{{7.1}{12}{\hskip -1em.~Implementation details and complexity of \algname {}}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{12}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}\hskip -1em.\nobreakspace  {}Properties of GASP{} with Absolute Maximum linkage}{12}{subsection.7.2}}
\newlabel{sec:appendix_abs_max}{{7.2}{12}{\hskip -1em.~Properties of \algname {} with Absolute Maximum linkage}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Remark on graph notation}{12}{section*.13}}
\@writefile{brf}{\backcite{wolf2018mutex}{{12}{7.2}{section*.13}}}
\@writefile{loe}{\contentsline {prop}{\numberline {7.1}Proposition}{12}{prop.7.1}}
\newlabel{prop:equiv_MWS}{{7.1}{12}{}{prop.7.1}{}}
\@writefile{brf}{\backcite{wolf2018mutex}{{12}{7.1}{prop.7.1}}}
\newlabel{eq:def_abs_max}{{7}{12}{}{equation.7.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Implementation of GASP{}, generalized algorithm for signed graph partitioning\relax }}{13}{algorithm.2}}
\newlabel{detailed_alg}{{2}{13}{Implementation of \algname {}, generalized algorithm for signed graph partitioning\relax }{algorithm.2}{}}
\@writefile{brf}{\backcite{wolf2018mutex}{{13}{3}{algorithm.3}}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Mutex Watershed Algorithm proposed by \cite  {wolf2018mutex}\relax }}{13}{algorithm.3}}
\newlabel{alg:mutex_watershed}{{3}{13}{Mutex Watershed Algorithm proposed by \cite {wolf2018mutex}\relax }{algorithm.3}{}}
\@writefile{loe}{\contentsline {prop}{\numberline {7.2}Proposition}{13}{prop.7.2}}
\newlabel{prop:abs_max_cannot_link_property}{{7.2}{13}{}{prop.7.2}{}}
\citation{ailon2008aggregating}
\citation{finkel2008enforcing}
\citation{andres2012globally}
\citation{saalfeld2012elastic}
\citation{ronneberger2015u}
\citation{cciccek20163d}
\citation{funke2018large}
\citation{lee2017superhuman}
\citation{funke2018large}
\citation{lee2017superhuman}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \textbf  {Left:} Example of edge contraction. First row: original graph $\mathcal  {G}$; clustering $\Pi $ (gray shaded areas) with dashed edges on cut; cannot-link constraints (violet bars). Second row: contracted graph $\mathaccentV {tilde}07E{\mathcal  {G}}_\Pi $. In step ii), edge $e_{uv}$ is contracted and node $v$ deleted from $\mathaccentV {tilde}07E{\mathcal  {G}}_\Pi $. In step iii), double edges $e_{tu}$ and $e_{tv}$ resulting from the edge contraction are replaced by a single edge with updated interaction. \textbf  {Right:} The table lists the update rules $f(\mathaccentV {tilde}07E{w}_1, \mathaccentV {tilde}07E{w}_2)$ associated to the linkage criteria of Table \ref  {tab:linkage-criteria} and that are used to efficiently update the interactions between clusters.\relax }}{14}{figure.caption.9}}
\newlabel{fig:edge_contraction_and_contr_graph}{{6}{14}{\textbf {Left:} Example of edge contraction. First row: original graph $\mathcal {G}$; clustering $\Pi $ (gray shaded areas) with dashed edges on cut; cannot-link constraints (violet bars). Second row: contracted graph $\tilde {\mathcal {G}}_\Pi $. In step ii), edge $e_{uv}$ is contracted and node $v$ deleted from $\tilde {\mathcal {G}}_\Pi $. In step iii), double edges $e_{tu}$ and $e_{tv}$ resulting from the edge contraction are replaced by a single edge with updated interaction. \textbf {Right:} The table lists the update rules $f(\tilde {\cost }_1, \tilde {\cost }_2)$ associated to the linkage criteria of Table \ref {tab:linkage-criteria} and that are used to efficiently update the interactions between clusters.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}\hskip -1em.\nobreakspace  {}Predicting signed edge weights with a CNN}{14}{subsection.7.3}}
\@writefile{brf}{\backcite{ailon2008aggregating}{{14}{7.3}{subsection.7.3}}}
\@writefile{brf}{\backcite{finkel2008enforcing,andres2012globally}{{14}{7.3}{subsection.7.3}}}
\newlabel{eq:mappings}{{9}{14}{\hskip -1em.~Predicting signed edge weights with a CNN}{equation.7.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}\hskip -1em.\nobreakspace  {}Neuron segmentation and compared methods}{14}{subsection.7.4}}
\newlabel{sec:cremi_details}{{7.4}{14}{\hskip -1em.~Neuron segmentation and compared methods}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Training details}{14}{section*.15}}
\@writefile{brf}{\backcite{saalfeld2012elastic}{{14}{7.4}{section*.15}}}
\@writefile{brf}{\backcite{ronneberger2015u, cciccek20163d}{{14}{7.4}{section*.15}}}
\@writefile{brf}{\backcite{funke2018large}{{14}{7.4}{section*.15}}}
\@writefile{brf}{\backcite{lee2017superhuman}{{14}{7.4}{section*.15}}}
\@writefile{brf}{\backcite{funke2018large}{{14}{7.4}{section*.15}}}
\@writefile{brf}{\backcite{lee2017superhuman}{{14}{7.4}{section*.15}}}
\@writefile{toc}{\contentsline {paragraph}{THRESH and WSDT}{14}{section*.16}}
\citation{keuper2015efficient}
\citation{kernighan1970efficient}
\citation{beier2016efficient}
\citation{wolf2018mutex}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces GASP{} with \emph  {AbsMax} linkage: Example representing the only case of edge contraction $e_{uv}$ that would introduce a positive attractive interaction between two constrained clusters. Note this can actually never happen with an \emph  {AbsMax} linkage, because edge $e_{ut}$ has a lower absolute priority as compared to $e_{uv}$, so clusters $u$ and $t$ cannot have been constrained before $u$ and $v$ are merged. \relax }}{15}{figure.caption.14}}
\newlabel{fig:abs_max_proof_example}{{7}{15}{\algname {} with \emph {AbsMax} linkage: Example representing the only case of edge contraction $e_{uv}$ that would introduce a positive attractive interaction between two constrained clusters. Note this can actually never happen with an \emph {AbsMax} linkage, because edge $e_{ut}$ has a lower absolute priority as compared to $e_{uv}$, so clusters $u$ and $t$ cannot have been constrained before $u$ and $v$ are merged. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-step pipelines}{15}{section*.17}}
\@writefile{brf}{\backcite{keuper2015efficient,kernighan1970efficient}{{15}{7.4}{section*.17}}}
\@writefile{brf}{\backcite{beier2016efficient}{{15}{7.4}{section*.17}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}\hskip -1em.\nobreakspace  {}GASP{} on the full CREMI dataset}{15}{subsection.7.5}}
\newlabel{sec:appendix_exps_full_cremi}{{7.5}{15}{\hskip -1em.~\algname {} on the full CREMI dataset}{subsection.7.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Pre-merge processing}{15}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Removing small segments}{15}{section*.19}}
\@writefile{toc}{\contentsline {paragraph}{Enforcing local merge}{15}{section*.20}}
\citation{levinkov2017comparative}
\citation{wolf2018mutex}
\citation{keuper2015efficient}
\citation{cremiChallenge}
\citation{arganda2015crowdsourcing}
\citation{cremiChallenge}
\citation{arganda2015crowdsourcing}
\citation{perlin2001noise}
\citation{perlin1985image}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{liu2018affinity}
\citation{wolf2018mutex}
\citation{dice1945measures}
\citation{sorensen1948method}
\@writefile{brf}{\backcite{wolf2018mutex}{{16}{7.5}{section*.20}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}\hskip -1em.\nobreakspace  {}GASP{} sensitivity to noise: adding artifacts to CNN predictions}{16}{subsection.7.6}}
\newlabel{sec:appendix_noise_gen}{{7.6}{16}{\hskip -1em.~\algname {} sensitivity to noise: adding artifacts to CNN predictions}{subsection.7.6}{}}
\newlabel{eq:noise_biased_predictions}{{10}{16}{\hskip -1em.~\algname {} sensitivity to noise: adding artifacts to CNN predictions}{equation.7.10}{}}
\@writefile{brf}{\backcite{perlin2001noise}{{16}{4}{Hfootnote.4}}}
\@writefile{brf}{\backcite{perlin1985image}{{16}{4}{Hfootnote.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}\hskip -1em.\nobreakspace  {}Fine-tuning the GMIS pipeline on CityScapes}{16}{subsection.7.7}}
\newlabel{sec:appendix_cityscapes}{{7.7}{16}{\hskip -1em.~Fine-tuning the GMIS pipeline on CityScapes}{subsection.7.7}{}}
\@writefile{brf}{\backcite{liu2018affinity}{{16}{7.7}{subsection.7.7}}}
\@writefile{brf}{\backcite{liu2018affinity}{{16}{7.7}{subsection.7.7}}}
\@writefile{brf}{\backcite{liu2018affinity}{{16}{7.7}{subsection.7.7}}}
\@writefile{brf}{\backcite{liu2018affinity}{{16}{7.7}{subsection.7.7}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{16}{7.7}{subsection.7.7}}}
\@writefile{brf}{\backcite{dice1945measures,sorensen1948method}{{16}{7.7}{subsection.7.7}}}
\citation{liu2018affinity}
\@writefile{brf}{\backcite{levinkov2017comparative}{{17}{\caption@xref {??}{ on input line 214}}{table.caption.21}}}
\@writefile{brf}{\backcite{wolf2018mutex}{{17}{\caption@xref {??}{ on input line 215}}{table.caption.21}}}
\@writefile{brf}{\backcite{keuper2015efficient}{{17}{\caption@xref {??}{ on input line 217}}{table.caption.21}}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performances achieved by different versions of GASP{} on the CREMI 2016 training set. CREMI-Score \cite  {cremiChallenge}, is given by a combination of the Adapted Rand-Score (Rand-Score) and the Variation of Information Score for under-clustering (VI-merge) and over-clustering (VI-split) \cite  {arganda2015crowdsourcing}. CLC stands for cannot-link constraints. For all algorithms, the chosen value of bias parameter was $\beta = 0$. We used a machine with CPU Intel(R) Xeon(R) E7-4870 @ 2.40GHz for our comparison experiments.\relax }}{17}{table.caption.21}}
\@writefile{brf}{\backcite{cremiChallenge}{{17}{6}{table.caption.21}}}
\@writefile{brf}{\backcite{arganda2015crowdsourcing}{{17}{6}{table.caption.21}}}
\newlabel{tab:extended_results_cremi}{{6}{17}{Performances achieved by different versions of \algname {} on the CREMI 2016 training set. CREMI-Score \cite {cremiChallenge}, is given by a combination of the Adapted Rand-Score (Rand-Score) and the Variation of Information Score for under-clustering (VI-merge) and over-clustering (VI-split) \cite {arganda2015crowdsourcing}. CLC stands for cannot-link constraints. For all algorithms, the chosen value of bias parameter was $\beta = 0$. We used a machine with CPU Intel(R) Xeon(R) E7-4870 @ 2.40GHz for our comparison experiments.\relax }{table.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces CNN predictions on a slice of the CREMI neuron segmentation challenge with and without additional noise. (\textbf  {a}) Raw data (\textbf  {b}) Original CNN predictions $F(x)$, where blue pixels represent boundary evidence (\textbf  {c}) Under-clustering biased version $\mathaccentV {tilde}07E{F}_{+}(x;\mathcal  {K})$ of the predictions defined in Eq. \ref  {eq:noise_biased_predictions} with $\mathcal  {K}=8$ (\textbf  {d}) Over-clustering biased version $\mathaccentV {tilde}07E{F}_{-}(x;\mathcal  {K})$. Long-range predictions are not shown. \relax }}{17}{figure.caption.22}}
\newlabel{fig:noisy_affs}{{8}{17}{CNN predictions on a slice of the CREMI neuron segmentation challenge with and without additional noise. (\textbf {a}) Raw data (\textbf {b}) Original CNN predictions $F(x)$, where blue pixels represent boundary evidence (\textbf {c}) Under-clustering biased version $\tilde {F}_{+}(x;\mathcal {K})$ of the predictions defined in Eq. \ref {eq:noise_biased_predictions} with $\mathcal {K}=8$ (\textbf {d}) Over-clustering biased version $\tilde {F}_{-}(x;\mathcal {K})$. Long-range predictions are not shown. \relax }{figure.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Average Precision (AP) scores achieved by different versions of GASP{} and chosen bias parameters $\beta $ on the cityscapes validation set. A bias value $\beta =0$ returns one single cluster. CLC stands for cannot-link constraints\relax }}{17}{figure.caption.22}}
\newlabel{tab:extended_results_cityscapes_val}{{7}{17}{Average Precision (AP) scores achieved by different versions of \algname {} and chosen bias parameters $\beta $ on the cityscapes validation set. A bias value $\beta =0$ returns one single cluster. CLC stands for cannot-link constraints\relax }{figure.caption.22}{}}
\@writefile{brf}{\backcite{liu2018affinity}{{17}{7.7}{subsection.7.7}}}
