% !TEX root = ../agglo_clust_review.tex

\section{Related work} \label{sec:related_work}
% In recent years there has been considerable progress in pixel-level understanding tasks of computer vision, such as semantic segmentation \cite{long2015fully,kong2018recurrent,chen2018deeplab}, boundary detection \cite{arbelaez2011contour,xie2015holistically,maninis2018convolutional}, %, optical flow \cite{weinzaepfel2013deepflow,dosovitskiy2015flownet},
Recent progress in semantic segmentation \cite{long2015fully,kong2018recurrent,chen2018deeplab}, boundary detection \cite{arbelaez2011contour,xie2015holistically,maninis2018convolutional}, %, optical flow \cite{weinzaepfel2013deepflow,dosovitskiy2015flownet},
and pose estimation \cite{wei2016convolutional,cao2017realtime} was mostly associated with new key ideas and architectures in deep learning, like dilated convolutions \cite{yu2015multi,chen2018deeplab} or ``hourglass'' architectures with skip connections \cite{ronneberger2015u,paszke2016enet} increasing the receptive field of view of the model.

\textbf{Proposal-based methods} decompose the instance segmentation task into two steps that consists in generating object proposals and assigning to each bounding box a class and a binary segmentation mask \cite{he2017mask,yang2012layered,li2017fully,ladicky2010and,hariharan2014simultaneous,chen2015multi,dai2016instance,liang2016reversible}. 
% Here I could have more: \cite{pinheiro2015learning,pinheiro2016learning,hu2017fastmask} and probably something more in Deep coloring
% \item \emph{More references:} or generate generic proposal segments and then label each one with a semantic detector \cite{hariharan2014simultaneous,chen2015multi,hariharan2015hypercolumns,dai2015convolutional,uhrig2016pixel,he2017mask}.
They commonly rely on {Faster-RCNN}~\cite{ren2015faster} and can be trained end-to-end using non-maximum suppression. Other methods use instead recurrent models to sequentially generate instances one-by-one \cite{romera2016recurrent,ren2017end}.

\textbf{Proposal-free methods} adopt a bottom-up approach by directly grouping pixels into instances. For example, the approach proposed in \cite{kirillov2017instancecut} uses a combinatorial framework for joint segmentation; SGN \cite{liu2017sgn} sequentially group pixels into lines and then instances;
% encode instance relationships to classes and exploit the boundary information  \cite{jin2016object}; 
a watershed transform is learned in \cite{bai2017deep} by also predicting its gradient direction, whereas the template matching \cite{uhrig2016pixel} deploys scene depth information.
Others use metric learning to predict high-dimensional associative pixel embeddings that map pixels of the same instance close to each other, while mapping pixels belonging to different instances further apart \cite{fathi2017semantic,newell2017associative,de2017semantic,novotny2018semi,kulikov2018instance}. 
% \TODO{What about \cite{liang2018proposal} using spectral clustering?}. 
Final instances are then retrieved by applying a clustering algorithm, like in the end-to-end trainable mean-shift pipeline of \cite{kong2018recurrent}.
%  and introducing a Ô¨Åxed number of labels (colors) and then dynamically assigning object instances to those labels during training (coloring) \cite{kulikov2018instance}

\textbf{Edge detection} also experienced recent noticeable progress \cite{xie2015holistically,kokkinos2015pushing}. This is nicely illustrated by the evolution of neuron segmentation for connectomics, a field of neuroscience we also address in our experiments. CNNs were introduced to this application in \cite{jain2007supervised}, but further progress in deep learning and data augmentation lead to much more refined boundary detection \cite{lee2017superhuman,meirovitch2016multi,ciresan2012deep}. 
% or trained an end-to-end watershed region growing algorithm \cite{wolf2017learned}.  
Subsequent postprocessing and superpixel-merging outputs final instances. Some use loopy graphs \cite{kaynig2015large,krasowski2015improving} or trees \cite{meirovitch2016multi,liu2016sshmt,liu2014modular,funke2015learning,uzunbas2016efficient} to represent the region merging hierarchy. The lifted multicut \cite{beier2017multicut} formulates the problem in a combinatorial framework, while flood-filling networks \cite{januszewski2018high} eliminate superpixels by training a recurrent CNN to perform region growing one region at the time. A structured learning approach was also proposed in \cite{funke2018large,turaga2009maximin}.
% \SOURCE{SSHMT} Check Funke proposals

\textbf{Agglomerative graph clustering} has often been applied to instance segmentation \cite{ren2013image,liu2016image,salembier2000binary}, because of its efficiency as compared to other top-down approaches like graph cuts. 
Novel termination criteria and merging strategies have often been proposed: the agglomeration in \cite{malmberg2011generalized} deploys fixed sets of merge constraints; ultrametric contour maps \cite{arbelaez2011contour} combine an oriented watershed transform with an edge detector, so that superpixels are merged until the ultrametric distance exceeds a learned threshold; the popular graph-based method \cite{felzenszwalb2004efficient} stops the agglomeration when the merge costs exceed a measure of quality for the current clusters; whereas the optimization approach in \cite{kiran2014global} performs greedy merge decisions that minimize a certain energy. \TODO{Roman, Cocoons, Benjamin}
On the other hand, several segmentation pipelines use classical HC linkage criteria, e.g. average linkage \cite{liu2018affinity,lee2017superhuman} or median \cite{funke2018large}. The GALA algorithm in \cite{nunez2013machine,knowles2016rhoananet} features instead a linkage learned by a neural network.

\textbf{Clustering of signed graphs} is another important line of research with the goal of balancing a graph with both attractive and repulsive cues. Finding an optimally balanced clustering has a long history in the field of combinatorial optimization \cite{grotschel1989cutting,grotschel1990facets,chopra1993partition}. %and can be done without the need to specify a termination criterion. 
The name \emph{correlation clustering} was coined by \cite{bansal2004correlation} that also showed its NP-hardness, while its connection with graph multicuts was made by \cite{demaine2006correlation}. Modern integer linear programming solvers can tackle problem instances of considerable size \cite{andres2012globally}, but accurate approximations \cite{pape2017solving,beier2016efficient,yarkony2012fast}, greedy heuristic algorithms \cite{levinkov2017comparative,wolf2018mutex,keuper2015efficient,kardoostsolving} and persistence criteria \cite{lange2018partial,lange2018combinatorial} have been proposed for even larger problems.
\TODO{Cannot-link constraints!}
% Missing: Funke proposals, discussion about cannot-link constraints!!!

\UPDATE{This work combines the clustering algorithms proposed in \cite{levinkov2017comparative,wolf2018mutex,keuper2015efficient} in a simple generalized framework for agglomerative clustering and evaluate them on an instance segmentation task by adopting ideas and CNN architectures from \cite{liu2018affinity,wolf2018mutex,funke2018large} to predict long-range relationships between pixels and boundary evidence between instances.}
