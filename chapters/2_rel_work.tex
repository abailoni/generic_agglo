% !TEX root = ../agglo_clust_review.tex

\section{Related work}

% \begin{itemize}

% \item Contributions: (and mention introductory Figure)
% \begin{itemize}
% \item We generalize the hierarchical agglomerative clustering framework to graphs with positive and negative edge attributes
% \item We review both standard hierarchical clustering algorithms and signed graph partitioning algorithms by showing how they can be described in this simplified framework (and as special cases of the general algorithm presented in Sec. \ref{sec:algorithm})
% \item we introduce and test new variations of these algorithms for signed graphs
% \end{itemize}
% \end{itemize}

% \begin{itemize}
% \item Motivation 2:
% \begin{itemize}
% \item HC has been always really popular in computer vision: \emph{Instance segmentation} is a task of computer vision that involves the detection of all objects in an image by performing pixel-level segmentation of each instance.   
% \item Since conventional unsigned graph agglomerative clustering algorithms, such as the well-known linkage methods [1], usually are quite sensitive to noise and outliers [1] (As their affinities are directly computed using pairwise distances between samples), most methods were based on multi-step pipelines first predicting superpixels (to reduce noise).
% \item These methods are still commonly used for specific instance segmentation applications: connectomics, build merge-tree 
%  %https://arxiv.org/pdf/1208.5092.pdf
% \item still commonly used for building merge-trees, give recent examples in connectomics (Sebastian Seung, etc...) % https://arxiv.org/abs/1608.04051
% \item Graph clustering algorithms recently gained more and more popularity with the advent of deep-learning and its application to instance segmentation
% \item Deep convolutional neural nets (CNNs) has been successfully applied to a wide range of tasks of computer vision and pixel-level image understanding, such as \UPDATE{boundary detection \cite{arbelaez2011contour,xie2015holistically,maninis2018convolutional}, semantic segmentation \cite{long2015fully,chen2018deeplab,kong2018recurrent}, optical flow \cite{weinzaepfel2013deepflow,dosovitskiy2015flownet}, and pose estimation \cite{wei2016convolutional,cao2017realtime}.}
% % \SOURCE{\cite{kong2018recurrent}}

% \item Currently there are two successful approaches used for proposal-free bottom-up instance segmentation: the first ones predict associative pixel embedding vectors, where the goal is to have a CNN predicting embedding vectors that are similar only for pixels in the same instance \cite{kong2018recurrent,fathi2017semantic,newell2017associative,de2017semantic}; in the second type of approaches, for every pixel a set of neighboring pixels is fixed  (not necesarelly limited to the direct neighbors) and a CNN then predicts affinities, that represent how likely it is for each of neighboring pixels to be in the same instance \cite{liu2018affinity,wolf2018mutex,xie2015holistically}

% \item Both approaches requires a final clustering algorithm to output the final instance segmentation 

%  and we can define a grid graph, where each vertex represents a pixel, with short- and long-range edge connections 

%  and find the final instance segmentation by using a graph clustering algorithm.

% \item GMIS achieved 
% comparable results to methods based on object 
% proposal like Mask-R-CNN on natural images 
% (Cityscapes); MWS achieved SOA results on a 
% biological images (ISBI)

% \item Out contributions:

% \begin{itemize}
%  \item we compare different types of agglomeration clustering on a pixel grid graph with short and long-range connections, focusing on aspects like efficieny, robustness (and quality of the final clustering in terms of multicut energy) \textbf{Comparison paper}
%  \item \textit{if we get good scores on CREMI dataset}: we show that also on neuro-data it is worth to skip the hand-crafted superpixel step and compute the final segmentation directly from the CNN affinities (MWS already showed it on the ISBI dataset): main selling point is again the almost-parameter free feature of the method
% \end{itemize}
% \end{itemize}
% \end{itemize}


% More details about instance segmentation (move to related work)
% \begin{itemize}


% \item 
% \item Many recent successful methods for instance segmentation %, usually named \emph{proposal-based methods}, 
%  use heuristic approaches that start from the classical computer vision task of \emph{object detection}, where the goal is to localize each objects using a bounding box, and then predict both a pixel-level mask of each instance and a semantic label (classify the objects in the bounding box). \TODO{Is it really general enough as a description...?} \cite{yang2012layered,ladicky2010and,hariharan2014simultaneous,chen2015multi,dai2016instance,liang2016reversible,he2017mask}
% \item While effective, these approaches are \UPDATE{somewhat unsatisfying (copy/paste, change: present some faults)} because:
% \begin{itemize}
% \item they rely on the object detector and non-maximum suppression heuristics to count how many instances are present in the image, 
% \item they tend to underperform in cluttered scenes, since instance assignment is often carried out independently for each detection; 
% \item they are not usable for wiry or articulated objects (e.g. in the field of connectomics and neuron segmenation from electron microscopy images) 
% \item their architecture do not usually prevent a pixel to be shared between multiple instances
% \item they do not scale up well since each proposal has to be singularly processed by the CNN. 
% % \item (difficult to train in an end-to-end manner: interface between instance segmentation and detection is non-differentiable)
% % \item (architecture is complex and hard to tune and “debug”...?)
% \end{itemize}

% \item Currently there are two successful approaches used for proposal-free (make brief intro) instance segmentation: the first ones predict associative pixel embedding vectors, where the goal is to have a CNN predicting embedding vectors that are similar only for pixels in the same instance \cite{kong2018recurrent,fathi2017semantic,newell2017associative,de2017semantic}; in the second type of approaches, for every pixel a set of neighboring pixels is fixed  (not necesarelly limited to the direct neighbors) and a CNN then predicts affinities, that represent how likely it is for each of neighboring pixels to be in the same instance \cite{liu2018affinity,wolf2018mutex,xie2015holistically}

% \item Both approaches requires a final clustering algorithm to output the final instance segmentation 

%  and we can define a grid graph, where each vertex represents a pixel, with short- and long-range edge connections 

%  and find the final instance segmentation by using a graph clustering algorithm.

% \item Recent work (\cite{wolf2018mutex}, more...) shows that it is better to use \textbf{repulsion and attraction} (directly predict it with the classifier, no need to define seeds or to fix a threshold given an hierarchy of clusters)
% \item solving correlation clustering problem (multicut) is too expensive for this application (even using recently proposed heuristics)
% \item our contributions:
% \begin{itemize}
% \item we propose a unified and simple formalization of Agglomerative Clustering and show how many of the recently proposed methods can be seen as special cases (focusing in particular on signed graphs) (\textbf{review and comparison paper})
% \item new clustering algorithms on signed graphs


% \end{itemize}
% \item on other datasets (connectomics) many methods are based on multi-step pipelines first predicting superpixels


% \end{itemize}


\begin{itemize}

% \item \emph{Possible intro about aggl. clustering:} \UPDATE{Hierarchical, agglomerative clustering is an important and well-established technique in unsupervised machine learning. Agglomerative clustering schemes start from the partition of the data set into singleton nodes and merge step by step the current pair of mutually closest nodes into a new node until there is one final node left, which comprises the entire data set. Various clustering schemes share this procedure as a common definition, but differ in the way in which the measure of inter-cluster dissimilarity is updated after each step. } %\cite{mullner2011modern}



% \item \emph{Other possible intro:} Many problems in computer vision involve clustering. Partitional clustering, such as k-means [1], determines all clusters at once, while agglomerative clustering [1] begins with a large number of small clusters, and iteratively selects two clusters with the largest affinity under some measures to merge, until some stopping condition is reached. Ag- glomerative clustering has been studied for more than half a century, and used in many applications [1], because it is conceptually simple and produces an informative hierar- chical structure of clusters. % FROM https://arxiv.org/pdf/1208.5092.pdf#page14

% \item Get more specific about graph (we do not cluster points in a space, but we cluster vertices with specified neighbors and interactions given by edges). 

% \item HC is a popular graph partitioning algorithm. Why: more efficient than divisive approaches and graph cuts; does not usually require to specify the final number of clusters (e.g. spectral clustering) or seed nodes (e.g. watershed, random walker). \UPDATE{And if the graph is not dense, HC algorithms scale up well with complexity}

% \item Motivation 1:

% \begin{itemize}

% \item all HC methods only talk about positive edge weights, but it is actually easy to generalize them for negative and attractive edges. 
% \item The standard version of HC is good for building hierarchy but problem with hierarchical merge-trees is how to choose one single threshold. Choose one single fixed density level. Ideally we would like to be able to cut the tree at different places to select our clusters.
% \item An important line of research is based on the observation that superior partitionings are obtained when the graph has both attractive and repulsive edges. Solutions that optimally balance attraction and repulsion do not require external stopping criteria such as predefined number of regions or seeds. This generalization leads to the NP-hard problem of correlation clustering or (synonymous) multicut (MC) partitioning. \emph{Alternative:} multicut / correlation clustering partitions vertices with both attractive and repulsive interactions encoded into the edges of a graph. Multicut has the great advantage that a “natural” partitioning of a graph can be found, without needing to specify a desired number of clusters, or a termination criterion, or one seed per region. Its great drawback is that its optimization is NP-hard.

% \end{itemize}



\item \textbf{Proposal based methods} 
\begin{itemize}
\item Generate region proposals or bounding boxes and classify the objects in the bounding box \cite{yang2012layered,ladicky2010and,hariharan2014simultaneous,chen2015multi,dai2016instance,liang2016reversible,he2017mask}; fully convolutional  box proposals \cite{li2017fully}. 
\item \emph{More references:} or generate generic proposal segments and then label each one with a semantic detector \cite{hariharan2014simultaneous,chen2015multi,hariharan2015hypercolumns,dai2015convolutional,uhrig2016pixel,he2017mask}.
\item these somehow still uses some kind of object detector (at least to find the number of objects), but they do not necessary analyse one proposal at the time: use an object detector to enumerate candidate instances and then perform pixel-level segmentation of each instance \cite{liang2018proposal,dai2016instance,li2017fully,liang2016reversible,arnab2017pixelwise} \TODO{check \cite{dai2016instance}}
% \SOURCE{\cite{kong2018recurrent}
\end{itemize}

\item \textbf{Proposal-free methods:} 
\begin{itemize}
\item box-free \cite{pinheiro2015learning,pinheiro2016learning,hu2017fastmask}; joint segmentation and instance labeling in a combinatorial framework \cite{kirillov2017instancecut}; recurrent models \cite{romera2016recurrent,ren2017end}; encode instance relationships to classes and exploit the boundary information  \cite{jin2016object}; sequential framework to gradually group from points to lines and then instances \cite{liu2017sgn}
% \item From end-to-end recurrent attention: \textit{Other approaches us- ing FCNs are proposal-free, but rely on a \textbf{bottom-up merging process}\textbf{}. Liang et al. [26] predict dense pixel prediction of ob- ject location and size, using clustering as a post-processing step. Uhrig et al. [41] present another approach based on FCNs, which is trained to produces a semantic segmen- tation as well as an instance-aware angle map, encoding the instance centroids. Post-processing based on template matching and instance fusion produces the instance identi- ties. Importantly, they also used ground-truth depth labels in training. Concurrent work [2, 19, 22] also explores a similar idea of using FCNs to output instance-sensitive embeddings.}
\item \textbf{Pixel embedding vectors}: 
\begin{itemize}
\item supervised vectors \cite{bai2017deep}, unsupervised vectors  \cite{kong2018recurrent,fathi2017semantic,newell2017associative,de2017semantic} of which \cite{kong2018recurrent} is trained end-to-end;  using scene depth information \cite{uhrig2016pixel}, \TODO{check \cite{sironi2014multiscale}} 
%    \TODO{more on pose estimation, vector distance transform, deep coloring?, ECCV semi-convolutions}

% different types of loss
% \item For each pixel in the image, a CNN predicts an embedding vector, such that pixels in the same instance are represented by the same vector. (\textit{by training a model that labels pixels with unit-length vectors that live in some ﬁxed dimension embedding space} \SOURCE{Rec. embeddings} ). \textit{With instance embedding, each object is assigned a “color” in a n-dimensional space. The network processes the image and produces a dense output, same size as the input image. Each pixel in the output of the network is a point in the embedding space. Pixels that belong to the same object are close in the embedding space while pixels that belong to different objects are distant in the embedding space. Parsing the image embedding space involves some sort of clustering algorithm.} \SOURCE{online} 

% \item Clustering methods: DBSCAN, more

\item \emph{Division by used clustering methods}: spectral clustering \cite{liang2018proposal}; mean-shift \cite{kong2018recurrent}; HDBSCAN \cite{}; seeds \cite{fathi2017semantic} \TODO{incomplete}
\end{itemize}

% \item Nevertheless, in both approaches we can define a grid graph (each vertex representing a pixel) with signed weights and find the final instance segmentation by using a graph partitioning algorithm:
% \begin{itemize}
% \item In Method 1, the graph is complete and the edge weights represent similarities between pairs of pixel embedding vectors. In most proposed approaches, embedding vectors representing distinct instances should be "distant enough" in the embedding space (a minimum distance is usually used during the training of the CNN). Thus, this threshold distance can be used to define signed affinities, representing attraction or repulsion between pairs of pixels;
% \item  In Method 2, the short- and long-range affinities predicted by the CNN are directly used as signed edge weights in the grid graph
% \end{itemize}



\item \textbf{Most related: predict short- and long-range affinities}. For each pixel we fix a set of neighboring pixels (not necessarily limited to the direct neighbors) and CNN predicts how likely it is for each pixel pair to be in the same instance \cite{liu2018affinity,wolf2018mutex,lee2017superhuman,xie2015holistically,Maire_2016_CVPR}. Mention that  \cite{liu2018affinity} is SoA on cityscapes for proposal-free methods and \cite{wolf2018mutex} is SoA on ISBI
% \item \cite{liu2018affinity} achieved comparable results to methods based on object proposal like Mask-R-CNN on natural images (Cityscapes); \cite{wolf2018mutex} achieved SOA results on a biological images (ISBI)
% \item Side comment: Even with pixel embedding vectors we can deduce affinities or signed costs
% (see MWS paper for a good review)

\end{itemize}



\item \textbf{Graph clustering related work}:
\begin{itemize}
    \item popular in image segmentation: \textbf{agglomerative clustering}.
    \item {\small More from \cite{mullner2011modern}: The seven most common methods are termed single, complete, average (UPGMA), weighted (WPGMA, McQuitty), Ward, centroid (UPGMC) and median (WPGMC) linkage (see Everitt et al., 2011, Table 4.1). They are implemented in standard numerical and statistical software such as R (R Development Core Team, 2011), MATLAB (The MathWorks, Inc., 2011), Mathematica (Wolfram Research, Inc., 2010), SciPy (Jones et al., 2001).}
    \item Why: more efficient than divisive approaches and graph cuts; does not usually require to specify the final number of clusters (e.g. spectral clustering) or seed nodes (e.g. watershed, random walker) 
    % old graph cut paper: \cite{shi2000normalized}
    % \item \emph{[other clustering methods working in an euclidean embedding space like mean-shift, K-means, Mixture models, BIRCH... can most probably be omitted]}
    \item many segmentation pipelines used to start the agglomeration from superpixels (SLIC, \cite{felzenszwalb2004efficient}, watershed) to reduce size of the problem 
    \item usually we build an hierarchy of clusters (or a \emph{merge-tree}): \TODO{find meaningful order}
    \item clustering paper suggested by Roman; Cocoons

 
% \item \textit{Some reference (atm in random order, to be continued)} 
\begin{itemize}

\item examples on natural images \cite{ren2013image,liu2016image}; ultra-metric contour map \cite{arbelaez2011contour};
\item \emph{Linkage criteria}: arithmetic average \cite{liu2018affinity,lee2017superhuman}; 
quantiles (median) \cite{funke2018large}; % and structured training 
   learned linkage criteria \cite{nunez2013machine}; 
\item An optimization perspective is taken in \cite{kiran2014global} %, which introduces h-increasing energy functions and builds the hierarchy incrementally such that merge decisions greedily minimize the energy
% Check MWS for description and more details
\item normalized cuts and combinatorial grouping \cite{arbelaez2014multiscale}
\item In \emph{connectomics}: \cite{liu2016sshmt}. Use loopy graphs \cite{kaynig2015large,krasowski2015improving} or trees \cite{liu2016sshmt,liu2014modular,funke2015learning,uzunbas2016efficient} to represent the region merging hierarchy. Using local \cite{liu2014modular,krasowski2015improving} or structured \cite{funke2015learning,uzunbas2016efficient} learning based methods % \SOURCE{SSHMT} Check Funke proposals
\end{itemize}
\item Methods finding a flat clustering (no hierarchy): efficient Graph-based Image segmentation \cite{felzenszwalb2004efficient} % defines a measure of quality for the current regions and stops when the merge costs would exceed this measure
\item \textbf{Signed graphs} \TODO{more}
\begin{itemize}
    % \item problem with hierarchical merge-trees is how to choose one single threshold. Choose one single fixed density level. Ideally we want to be able to cut the tree at different places to select our clusters. In density space, \cite{campello2013density}
    \item Multicut pipiline and heuristics \cite{beier2017multicut}... 
    % \item \textit{\textbf{Multicut} has the great advantage that a “natural” partitioning of a graph can be found, without needing to specify a desired number of clusters, or a termination criterion, or one seed per region. Its great drawback is that its optimization is NP-hard.} \SOURCE{MWS}.

\item hierarchy and multicut proposals \cite{funke2018candidate}
\item \textit{Must-not-link edges:} initially introduced as hard-constraints \cite{malmberg2011generalized}, then introduced dynamically \cite{wolf2018mutex,levinkov2017comparative}. 
\item Local-search approximations of MC: greeedy fixation and greedy additive edge contraction \cite{levinkov2017comparative}
\end{itemize}
\end{itemize}

\end{itemize}

